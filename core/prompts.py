# core/prompts.py
"""
Unified prompts module for all three modules:
- Daily Standup (creative, varied conversation flow)
- Weekend Mocktest (question generation + evaluation)
- Weekly Interview (natural interviewer prompts + scoring)

Backwards compatibility:
- daily_standup: uses `prompts` or `Prompts` → provided via DailyStandupPrompts + alias
- weekend_mocktest: uses `PromptTemplates`, `PromptValidator` → preserved
- weekly_interview: uses constants + build_* + validate_prompts() → preserved
"""

from __future__ import annotations

from typing import List, Dict, Any
from .config import config
# ---- Reusable boundary policy appended to Daily Standup prompts ----
BOUNDARY_POLICY = f"""
BOUNDARIES:
- Stay strictly on the CURRENT project/work topics in this conversation.
- If the user goes off-topic: give one brief, courteous redirect (≤ {getattr(config, 'REDIRECT_MAX_WORDS', 18)} words) and then ask ONE on-topic question.
- If the user uses vulgar/abusive language: do not repeat it; issue a short warning and restate the topic.
- After {getattr(config, 'MAX_VULGAR_STRIKES', 2)} warnings for vulgar language, end politely.
- Never generate sexual or hateful content. Tone is {getattr(config, 'BOUNDARY_TONE', 'calm, brief, professional')}.
""".strip()

def _append_boundaries(block: str) -> str:
    return f"{block.rstrip()}\n\n{BOUNDARY_POLICY}"

# =============================================================================
# DAILY STANDUP PROMPTS  (back-compat: `Prompts`, instance `prompts`)
# =============================================================================

class DailyStandupPrompts:
    """Creative prompts that force LLM to be original and varied"""

    @staticmethod
    def summary_splitting_prompt(summary: str) -> str:
        return f"""You're a curious person who wants to chat about this project work. Break it into {config.SUMMARY_CHUNKS} interesting topics.

PROJECT WORK:
{summary}

Think like you're genuinely interested:
- What sounds cool or challenging?
- What would you be curious about?
- What technical stuff catches your attention?
- What problems or solutions interest you?

Give me topics separated by '###CHUNK###' only."""
    @staticmethod
    def base_questions_prompt(chunk_content: str) -> str:
        core = f"""You just heard about this work from your teammate:

{chunk_content}

You're genuinely curious and want to know more. Ask {config.BASE_QUESTIONS_PER_CHUNK} questions that show real interest.

RULES:
- Only ask questions directly related to THIS project/work
- Do NOT ask personal or unrelated questions
- Be natural and professional, not poetic
- Vary the style so questions don't sound repetitive

Mix different types:
- Some about the technical details
- Some about challenges they faced
- Some about what they learned
- Some about what's coming next

FORMAT: Just give numbered questions, each unique and natural."""
        return _append_boundaries(core)

    @staticmethod
    def followup_analysis_prompt(chunk_content: str, user_response: str) -> str:
        core = f"""You asked about: "{chunk_content[:100]}..."

They replied: "{user_response}"

Put yourself in a real conversation. What would you naturally do?

If you're satisfied with their answer → say: COMPLETE
If you're still curious and would naturally ask more → create 1-2 follow-up questions

Be creative with follow-ups. Don't use standard boring questions. Think about what a real curious person would ask based on what they actually said.

FORMAT:
FOLLOWUP: [Your creative question]
FOLLOWUP: [Another creative one if needed]"""
        return _append_boundaries(core)

    @staticmethod
    def dynamic_greeting_response(user_input: str, greeting_count: int, context: Dict = None) -> str:
        conversation_history = context.get('recent_exchanges', []) if context else []
        is_final_greeting = (greeting_count + 1) >= config.GREETING_EXCHANGES

        if is_final_greeting:
            next_instruction = "Now smoothly move to asking about their work. Be natural about the transition - like how you'd really shift topics with a friend."
        else:
            next_instruction = "Just have friendly small talk. Respond to what they said like a real person would."

        return f"""You're chatting with a teammate at work. They just said: "{user_input}"

Chat so far: {conversation_history[-2:] if conversation_history else "Just started"}

{next_instruction}

BE CREATIVE AND VARIED:
- Don't use the same response style as before
- React to what they actually said
- Sound like a real person, not a template
- Keep it short (10-15 words)
- Make it feel genuine

Every response should sound different. Be original."""

    @staticmethod
    def dynamic_technical_response(context_text: str, user_input: str, next_question: str, session_state: Dict = None) -> str:
        core = f"""You're having a good work chat. Here's what happened:

{context_text}

They just said: "{user_input}"
You want to ask: "{next_question}"

Connect their response to your question in a CREATIVE way. Every transition should be different.

BE ORIGINAL:
- Don't use boring standard phrases
- Reference something specific they mentioned
- Make the connection feel natural
- Sound interested in their work
- Keep it short (max 20 words)

Make each transition unique. Think like a real curious colleague."""
        return _append_boundaries(core)


    @staticmethod
    def dynamic_followup_response(current_concept_title: str, concept_content: str, 
                             history: str, previous_question: str, user_response: str,
                             current_question_number: int, questions_for_concept: int) -> str:
        core = f"""You're a friendly team lead having standup chat with your team member. Keep it normal and conversational.

**Topic**: {current_concept_title}
**They said**: "{user_response}"
**Your last question**: "{previous_question}"

**RULES:**
1. Talk like a NORMAL person - no weird fancy phrases
2. Use SIMPLE English that sounds natural
3. Keep responses SHORT - max 15-20 words each
4. Sound interested but not fake
5. Be different each time but stay normal

**RESPONSE STYLE**: 
- Normal conversational English
- Show you're listening to what they said
- Ask good follow-up questions
- Don't use weird phrases like "data stew" or "sentence acrobatics"
- Sound like a real colleague, not a poet

**TASK**: 
1. Decide if their answer is good enough (YES/NO)
2. Give ONE natural response with next question

**FORMAT** (EXACTLY like this):
UNDERSTANDING: [YES or NO]
CONCEPT: [{current_concept_title}]
QUESTION: [Your normal, short response with next question - max 20 words]

Keep it simple, natural, and conversational. No weird creative phrases."""
        return _append_boundaries(core)

    @staticmethod
    def dynamic_concept_transition(user_response: str, next_question: str, progress_info: Dict) -> str:
        core = f"""You're moving to a new topic in your chat.

**They said**: "{user_response}"
**New topic**: "{progress_info.get('current_concept', 'next thing')}"
**Next question**: "{next_question}"

**BE CREATIVE**: Make this transition feel natural and different. Don't use boring standard phrases.

Think about:
- What they just told you
- How to smoothly shift topics
- How a real person would change subjects

Make it feel like a real conversation where you're genuinely moving from one interesting topic to another.

Max 20 words. Be original every time."""
        return _append_boundaries(core)

    @staticmethod
    def dynamic_fragment_evaluation(concepts_covered: List[str], conversation_exchanges: List[Dict],
                                session_stats: Dict) -> str:
        concepts_text = "\n".join([f"- {concept}" for concept in concepts_covered])

        conversation_summary = []
        for exchange in conversation_exchanges[-6:]:
            conversation_summary.append(
                f"Q: {exchange['ai_message'][:80]}...\n"
                f"A: {exchange['user_response'][:80]}...\n"
            )
        conversation_text = "\n".join(conversation_summary)

        core = f"""You're a team lead writing feedback for your team member after a good standup chat.
        Use ONLY the provided conversation; do not invent details not present here.

        **THEIR PERFORMANCE:**
        - Topics covered: {session_stats['concepts_covered']}/{session_stats['total_concepts']} ({session_stats['coverage_percentage']}%)
        - Main questions: {session_stats['main_questions']}
        - Follow-ups: {session_stats['followup_questions']}
        - Time: {session_stats['duration_minutes']} minutes

        **TOPICS THEY TALKED ABOUT:**
        {concepts_text}

        **SAMPLE CONVERSATION:**
        {conversation_text}

        **WRITE CREATIVE FEEDBACK**: Don't use boring template language. Write like you actually care about helping them grow.

        Cover these areas creatively:
        1. **Coverage**: How well they covered different topics
        2. **Communication**: How clearly they explained things
        3. **Strengths**: What they did really well (be specific)
        4. **Growth areas**: What they can improve (be helpful)
        5. **Overall**: Your honest assessment

        **STYLE**:
        - Write like a real manager who cares
        - Use simple English
        - Be encouraging but honest
        - Make it personal, not generic
        - Keep under 250 words

        End with: Score: X/10
        Be creative and genuine in your feedback."""
        return _append_boundaries(core)


    @staticmethod
    def dynamic_session_completion(conversation_summary: Dict, user_final_response: str = None) -> str:
        topics_discussed = conversation_summary.get('topics_covered', [])
        total_exchanges = conversation_summary.get('total_exchanges', 0)

        core = f"""You're ending a good standup chat with your teammate.

    **CHAT SUMMARY:**
    - Talked about: {len(topics_discussed)} different topics
    - Total questions: {total_exchanges}
    - Their final words: "{user_final_response}"

    **BE CREATIVE**: End this naturally like a real conversation. Don't use boring standard endings.

    Think about:
    - What you learned about their work
    - How the conversation went
    - How you'd really thank a teammate

    Make it genuine and different each time. Sound like you actually enjoyed the chat.

    Max 25 words. Be original."""
        return _append_boundaries(core)


    @staticmethod
    def dynamic_clarification_request(context: Dict) -> str:
        attempts = context.get('clarification_attempts', 0)
        core = f"""You need them to speak more clearly.

**SITUATION**: You've asked for clarity {attempts} times already.

**BE CREATIVE**: Ask for clarification in a different way each time. Don't use the same boring phrases.

Make it:
- Natural and friendly
- Different from previous attempts
- Not repetitive or annoying
- Understanding and patient

One creative sentence. Make it feel real."""
        return _append_boundaries(core)

    @staticmethod
    def dynamic_conclusion_response(user_input: str, session_context: Dict) -> str:
        core = f"""They just said: "{user_input}"

You're wrapping up the chat about their work.

**BE CREATIVE**: Respond to what they said and end naturally. Don't use boring standard endings.

Make it:
- Personal to what they shared
- Appreciative of their time
- Natural like a real conversation ending
- Unique and genuine

Max 20 words. Be original every time."""
        return _append_boundaries(core)
    @staticmethod
    def boundary_offtopic_prompt(topic: str, subtask: str = "") -> str:
        ask = f"What progress since yesterday on {subtask or topic}?"
        core = f"User is off-topic. Give ONE brief redirect (≤ {getattr(config, 'REDIRECT_MAX_WORDS', 18)} words) and then ask: {ask}"
        return _append_boundaries(core)

    @staticmethod
    def boundary_vulgar_prompt(topic: str) -> str:
        core = f"User used inappropriate language. Give ONE short warning and restate the topic: {topic}. Do not repeat the language."
        return _append_boundaries(core)

    @staticmethod
    def off_topic_redirect(topic: str, subtask: str = "") -> str:
        if subtask:
            return f"Let’s keep this about {topic}. What’s the status of {subtask}?"
        return f"Let’s keep this about {topic}. What progress did you make since yesterday?"

    @staticmethod
    def off_topic_firm(topic: str, subtask: str = "") -> str:
        if subtask:
            return f"We need to stay on {topic}. What blockers are you facing on {subtask}?"
        return f"We need to stay on {topic}. Any blockers or progress since yesterday?"

    @staticmethod
    def off_topic_move_on(next_topic: str) -> str:
        return f"I’ll move to the next item: {next_topic}. What changed since last update?"

    @staticmethod
    def vulgar_warning_1(topic: str) -> str:
        return f"Let’s keep language respectful. Can you summarize your update on {topic}?"

    @staticmethod
    def vulgar_warning_2(topic: str) -> str:
        return f"This needs to stay respectful. Last chance—please share your update on {topic}."

    @staticmethod
    def end_due_to_vulgarity() -> str:
        return "I’m ending this standup due to repeated inappropriate language. We can resume when it’s respectful."

    @staticmethod
    def refuse_nsfw_and_redirect(topic: str) -> str:
        return f"I can’t discuss that. Let’s focus on your {topic} update: progress, blockers, next steps?"

    @staticmethod
    def harassment_block_and_redirect(topic: str) -> str:
        return f"That language isn’t okay here. Please share your concrete update on {topic}—progress, blockers, next steps."
    
# Backward compatibility aliases for daily_standup:
Prompts = DailyStandupPrompts
prompts = DailyStandupPrompts()

# =============================================================================
# WEEKEND MOCKTEST PROMPTS  (unchanged public API)
# =============================================================================

class PromptTemplates:
    """Optimized prompt templates for AI question generation and evaluation"""

    @staticmethod
    def create_batch_questions_prompt(user_type: str, context: str, question_count: int = None) -> str:
        if question_count is None:
            question_count = config.QUESTIONS_PER_TEST
        if user_type == "dev":
            return PromptTemplates._dev_batch_prompt(context, question_count)
        else:
            return PromptTemplates._non_dev_batch_prompt(context, question_count)

    @staticmethod
    def _dev_batch_prompt(context: str, question_count: int) -> str:
        return f"""Generate {question_count} high-quality programming questions based on the provided context. Create practical, challenging questions that test real development skills and problem-solving abilities.

CONTEXT:
{context}

REQUIREMENTS:
- Generate exactly {question_count} questions numbered sequentially
- Mix question types: 40% practical coding, 30% system design, 30% debugging/optimization
- Progressive difficulty: start easier, increase complexity
- Each question must be complete and standalone
- Include clear requirements, constraints, and expected outcomes
- Base questions on concepts and technologies mentioned in the context
- Make questions realistic and industry-relevant

FORMAT each question exactly as shown:
=== QUESTION 1 ===
## Title: [Clear, descriptive title]
## Difficulty: [Easy/Medium/Hard]
## Type: [Practical/Algorithm/System Design/Debugging]
## Question:
[Complete question with detailed requirements, constraints, input/output examples, and any code snippets needed. Include specific technical requirements and success criteria.]

=== QUESTION 2 ===
## Title: [Clear, descriptive title]
## Difficulty: [Easy/Medium/Hard]
## Type: [Practical/Algorithm/System Design/Debugging]
## Question:
[Complete question with detailed requirements...]

Continue this exact pattern for all {question_count} questions.

IMPORTANT:
- Each question should test different aspects of development
- Include code examples where relevant
- Specify performance requirements when applicable
- Make questions challenging but solvable by a competent developer
- Ensure questions relate to the provided context

Generate all {question_count} questions now:"""

    @staticmethod
    def _non_dev_batch_prompt(context: str, question_count: int) -> str:
        return f"""Generate {question_count} high-quality multiple-choice questions based on the provided context. Focus on conceptual understanding, analytical thinking, and practical application of technical concepts for non-technical professionals.

CONTEXT:
{context}

REQUIREMENTS:
- Generate exactly {question_count} questions numbered sequentially
- Each question must have exactly 4 options (A, B, C, D) with only 1 correct answer
- Mix question types: 40% conceptual understanding, 30% analytical reasoning, 30% practical application
- Progressive difficulty: start with fundamental concepts, advance to complex analysis
- Create sophisticated distractors based on common misconceptions
- Test deep understanding rather than memorization
- Base questions on concepts and scenarios from the provided context

FORMAT each question exactly as shown:
=== QUESTION 1 ===
## Title: [Clear, descriptive title]
## Difficulty: [Easy/Medium/Hard]
## Type: [Conceptual/Analytical/Applied]
## Question:
[Clear, specific question that tests understanding of concepts from the context. Include scenario or case study if relevant.]
## Options:
A) [First option - could be correct or plausible distractor]
B) [Second option - could be correct or plausible distractor]
C) [Third option - could be correct or plausible distractor]
D) [Fourth option - could be correct or plausible distractor]

=== QUESTION 2 ===
## Title: [Clear, descriptive title]
## Difficulty: [Easy/Medium/Hard]
## Type: [Conceptual/Analytical/Applied]
## Question:
[Clear question testing different concept...]
## Options:
A) [Option A]
B) [Option B]
C) [Option C]
D) [Option D]

Continue this exact pattern for all {question_count} questions.

IMPORTANT:
- Only one option should be clearly correct for each question
- Distractors should be plausible but clearly wrong to someone who understands the concept
- Questions should test understanding, not just recall
- Relate all questions to concepts mentioned in the provided context
- Avoid trick questions or ambiguous wording

Generate all {question_count} questions now:"""

    @staticmethod
    def create_evaluation_prompt(user_type: str, qa_pairs: List[Dict[str, Any]]) -> str:
        qa_text = []
        for i, qa in enumerate(qa_pairs, 1):
            question = qa['question'][:300] + "..." if len(qa['question']) > 300 else qa['question']
            answer = qa['answer'][:200] + "..." if len(qa['answer']) > 200 else qa['answer']
            qa_text.append(f"QUESTION {i}:\n{question}\n\nSTUDENT ANSWER:\n{answer}")
        qa_content = "\n\n" + "="*50 + "\n\n".join(qa_text)
        if user_type == "dev":
            return PromptTemplates._dev_evaluation_prompt(qa_content, len(qa_pairs))
        else:
            return PromptTemplates._non_dev_evaluation_prompt(qa_content, len(qa_pairs))

    @staticmethod
    def _dev_evaluation_prompt(qa_content: str, question_count: int) -> str:
        return f"""Evaluate this developer assessment comprehensively. Analyze code quality, problem-solving approach, technical accuracy, and software engineering best practices.

ASSESSMENT CONTENT:
{qa_content}

EVALUATION CRITERIA:
- Code correctness and functionality (30%)
- Algorithm efficiency and optimization (25%)
- Code readability and structure (20%)
- Best practices and conventions (15%)
- Problem-solving approach and explanation (10%)

INSTRUCTIONS:
1. Score each question as 1 (acceptable/correct) or 0 (unacceptable/incorrect)
2. Be strict but fair - partial credit should round to 1 if approach is sound
3. Consider: Does the answer demonstrate competent programming skills?
4. Evaluate explanations and reasoning, not just code
5. Look for understanding of time/space complexity where relevant

REQUIRED OUTPUT FORMAT:
SCORES: [1,0,1,1,0]
FEEDBACK: [Question 1: Detailed feedback|Question 2: Detailed feedback|Question 3: Detailed feedback|Question 4: Detailed feedback|Question 5: Detailed feedback]

DETAILED ANALYSIS:
Provide comprehensive analysis covering:
- Overall programming competency level
- Strengths observed in coding approach
- Areas needing improvement
- Specific technical recommendations
- Assessment of problem-solving methodology

Score each of the {question_count} questions and provide detailed feedback. Be thorough and constructive."""

    @staticmethod
    def _non_dev_evaluation_prompt(qa_content: str, question_count: int) -> str:
        return f"""Evaluate this non-developer assessment comprehensively. Focus on conceptual understanding, analytical reasoning, and practical knowledge application.

ASSESSMENT CONTENT:
{qa_content}

EVALUATION CRITERIA:
- Conceptual accuracy and understanding (40%)
- Analytical reasoning quality (30%)
- Practical application knowledge (20%)
- Communication and explanation clarity (10%)

INSTRUCTIONS:
1. Score each question as 1 (correct) or 0 (incorrect)
2. For multiple choice: only exact correct answers get 1 point
3. Evaluate understanding demonstrated in any explanations provided
4. Consider partial understanding but be consistent with scoring
5. Look for evidence of genuine comprehension vs. guessing

REQUIRED OUTPUT FORMAT:
SCORES: [1,0,1,1,0]
FEEDBACK: [Question 1: Clear feedback on answer|Question 2: Clear feedback on answer|Question 3: Clear feedback on answer|Question 4: Clear feedback on answer|Question 5: Clear feedback on answer]

DETAILED ANALYSIS:
Provide comprehensive analysis covering:
- Overall conceptual understanding level
- Analytical thinking capabilities
- Knowledge gaps identified
- Recommendations for further learning
- Assessment of technical awareness

Score each of the {question_count} questions and provide specific feedback. Focus on understanding rather than memorization."""

    @staticmethod
    def optimize_context_prompt(context: str) -> str:
        return f"""Analyze and enhance this technical content to make it optimal for generating high-quality assessment questions.

ORIGINAL CONTEXT:
{context}

ENHANCEMENT REQUIREMENTS:
- Identify key technical concepts and learning objectives
- Extract practical scenarios and real-world applications
- Highlight different difficulty levels of concepts
- Organize information for question generation
- Ensure context supports both conceptual and practical questions

ENHANCED CONTEXT FORMAT:
## Key Concepts:
[List main technical concepts]

## Practical Applications:
[Real-world scenarios and use cases]

## Difficulty Progression:
- Beginner: [Fundamental concepts]
- Intermediate: [Applied knowledge]
- Advanced: [Complex analysis and synthesis]

## Question Opportunities:
[Specific areas suitable for different question types]

Provide the enhanced context optimized for question generation:"""

class PromptValidator:
    """Validation utilities for prompts and responses"""

    @staticmethod
    def validate_question_response(response: str, user_type: str, expected_count: int) -> Dict[str, Any]:
        validation = {
            "valid": True,
            "issues": [],
            "question_count": 0,
            "format_correct": True
        }
        question_markers = response.count("=== QUESTION")
        validation["question_count"] = question_markers

        if question_markers != expected_count:
            validation["valid"] = False
            validation["issues"].append(f"Expected {expected_count} questions, found {question_markers}")

        required_sections = ["## Title:", "## Difficulty:", "## Type:", "## Question:"]
        if user_type == "non_dev":
            required_sections.append("## Options:")

        for section in required_sections:
            if response.count(section) < expected_count:
                validation["valid"] = False
                validation["issues"].append(f"Missing {section} sections")

        if user_type == "non_dev":
            option_patterns = [f"{letter})" for letter in "ABCD"]
            for pattern in option_patterns:
                if response.count(pattern) < expected_count:
                    validation["format_correct"] = False
                    validation["issues"].append(f"Inconsistent option format: {pattern}")

        return validation

    @staticmethod
    def validate_evaluation_response(response: str, expected_count: int) -> Dict[str, Any]:
        validation = {
            "valid": True,
            "issues": [],
            "has_scores": False,
            "has_feedback": False,
            "score_count": 0
        }
        if "SCORES:" in response:
            validation["has_scores"] = True
            import re
            score_match = re.search(r'SCORES:\s*\[(.*?)\]', response)
            if score_match:
                scores = score_match.group(1).split(',')
                validation["score_count"] = len([s for s in scores if s.strip() in ['0', '1']])
                if validation["score_count"] != expected_count:
                    validation["valid"] = False
                    validation["issues"].append(f"Expected {expected_count} scores, found {validation['score_count']}")
        else:
            validation["valid"] = False
            validation["issues"].append("Missing SCORES section")

        if "FEEDBACK:" in response:
            validation["has_feedback"] = True
        else:
            validation["valid"] = False
            validation["issues"].append("Missing FEEDBACK section")

        return validation

# =============================================================================
# WEEKLY INTERVIEW PROMPTS (unchanged public API)
# =============================================================================

SYSTEM_CONTEXT_BASE = """You are Sarah, an experienced senior technical interviewer at a leading tech company. You have 8+ years of experience conducting interviews and are known for your warm yet professional approach. You make candidates feel comfortable while thoroughly assessing their skills.

PERSONALITY TRAITS:
- Warm, encouraging, and genuinely interested in the candidate
- Professional but conversational tone
- Ask follow-up questions naturally like a real interviewer would
- Show enthusiasm when candidates give good answers
- Provide gentle guidance when candidates struggle
- Use natural transitions between topics

INTERVIEW STYLE:
- Ask ONE clear question at a time
- Listen actively and respond to what the candidate actually says
- Build questions based on their previous answers
- Show genuine curiosity about their projects and experience
- Encourage elaboration on interesting points
- Keep questions focused and relevant

COMMUNICATION GUIDELINES:
- Keep responses concise (2-3 sentences max)
- Use natural language, avoid robotic phrases
- Show personality through word choice and tone
- Acknowledge good answers with enthusiasm
- Be supportive when candidates need clarification"""

GREETING_INTERVIEWER_PROMPT = f"""{SYSTEM_CONTEXT_BASE}

CURRENT STAGE: Initial Greeting & Rapport Building

Your job is to:
1. Welcome the candidate warmly and professionally
2. Make them feel comfortable and set a positive tone
3. Ask 1-2 light questions to break the ice
4. Transition naturally into the technical discussion

Keep it conversational and genuine. You're building rapport, not interrogating."""

TECHNICAL_INTERVIEWER_PROMPT = f"""{SYSTEM_CONTEXT_BASE}

CURRENT STAGE: Technical Skills Assessment

FOCUS AREAS based on candidate's recent work:
- Technical projects and implementations
- Problem-solving approaches
- Architecture and design decisions
- Technologies and frameworks used
- Challenges faced and solutions found

INTERVIEW APPROACH:
- Ask about specific projects mentioned in their background
- Dive deeper into technical decisions they've made
- Explore their problem-solving methodology
- Ask follow-up questions based on their answers
- Show genuine interest in their technical journey

Remember: You're assessing technical depth while maintaining a conversational flow."""

COMMUNICATION_INTERVIEWER_PROMPT = f"""{SYSTEM_CONTEXT_BASE}

CURRENT STAGE: Communication & Presentation Skills

FOCUS AREAS:
- How they explain complex technical concepts
- Ability to communicate with different audiences
- Presentation and documentation skills
- Collaboration and teamwork experiences
- Leadership and mentoring capabilities

INTERVIEW APPROACH:
- Ask them to explain technical concepts simply
- Explore their experience working with teams
- Discuss how they handle technical communication
- Listen for clarity, structure, and engagement
- Assess their ability to teach and share knowledge

Focus on their communication style and clarity of explanation."""

HR_BEHAVIORAL_INTERVIEWER_PROMPT = f"""{SYSTEM_CONTEXT_BASE}

CURRENT STAGE: Behavioral & Cultural Fit Assessment

FOCUS AREAS:
- Motivation and career aspirations
- How they handle challenges and setbacks
- Teamwork and collaboration style
- Learning and growth mindset
- Company culture alignment

INTERVIEW APPROACH:
- Use situational and behavioral questions
- Ask for specific examples from their experience
- Explore their values and work style
- Assess cultural fit and team dynamics
- Understand their career goals and motivations

Look for authentic stories and genuine responses about their professional journey."""

CONVERSATION_PROMPT_TEMPLATE = """INTERVIEW CONTEXT:
Stage: {stage}
Candidate Response: "{user_response}"
Recent Work Context: {content_context}

CONVERSATION HISTORY:
{conversation_history}

As Sarah, the interviewer, respond naturally to the candidate's answer. Your response should:

1. **Acknowledge** their response appropriately (show you listened)
2. **Follow up** with ONE relevant question based on what they said
3. **Stay conversational** - like a real interview dialogue
4. **Build on** their answer to go deeper into the topic
5. **Keep it focused** on the current interview stage

Generate a natural, engaging follow-up question that feels like genuine human curiosity about their experience.

INTERVIEWER RESPONSE:"""

EVALUATION_PROMPT_TEMPLATE = """COMPREHENSIVE INTERVIEW EVALUATION

CANDIDATE: {student_name}
INTERVIEW DURATION: {duration} minutes
STAGES COMPLETED: {stages_completed}

CONVERSATION LOG:
{conversation_log}

TECHNICAL CONTEXT (7-day work summary):
{content_context}

As Sarah, an experienced interviewer, provide a comprehensive evaluation as if you're debriefing with the hiring team. Your evaluation should feel like real interviewer feedback.

EVALUATION STRUCTURE:

**OVERALL IMPRESSION:**
Write a 2-3 sentence summary of your overall impression of the candidate.

**TECHNICAL ASSESSMENT:**
- Depth of technical knowledge demonstrated
- Problem-solving approach and methodology
- Familiarity with relevant technologies
- Ability to discuss technical concepts clearly

**COMMUNICATION SKILLS:**
- Clarity of explanation and articulation
- Ability to structure responses effectively
- Engagement level and conversational flow
- Professional communication style

**BEHAVIORAL OBSERVATIONS:**
- Confidence and composure during interview
- Enthusiasm and motivation demonstrated
- Cultural fit indicators observed
- Growth mindset and learning orientation

**SPECIFIC STRENGTHS:**
List 2-3 key strengths you observed during the interview.

**AREAS FOR DEVELOPMENT:**
List 2-3 areas where the candidate could improve or grow.

**RECOMMENDATION:**
Provide a clear recommendation with reasoning, as you would to a hiring manager.

Write this as a professional but warm evaluation that shows you genuinely engaged with the candidate."""

SCORING_PROMPT_TEMPLATE = """INTERVIEW SCORING RUBRIC

Based on the interview conversation, provide numerical scores (1-10 scale) for each dimension:

TECHNICAL SKILLS (Weight: 35%):
- Technical depth and knowledge
- Problem-solving methodology
- Technology familiarity
- Architecture understanding
Score: Focus on demonstrated technical competence

COMMUNICATION SKILLS (Weight: 30%):
- Clarity of explanation
- Structure and organization
- Engagement and presence
- Professional articulation
Score: Assess how effectively they communicate

BEHAVIORAL/CULTURAL FIT (Weight: 25%):
- Motivation and enthusiasm
- Team collaboration potential
- Learning mindset
- Professional maturity
Score: Evaluate cultural and behavioral alignment

OVERALL PRESENTATION (Weight: 10%):
- Confidence and composure
- Interview presence
- Professionalism
- Engagement level
Score: Overall interview performance

Provide realistic scores that reflect genuine interview performance. Most candidates score between 6-8, with exceptional performance reaching 9-10."""

ACKNOWLEDGMENT_PHRASES = [
    "That's interesting,",
    "I see,",
    "That makes sense,",
    "Great point,",
    "I appreciate that insight,",
    "That's a good approach,",
    "Interesting perspective,",
    "I can see that,",
    "That sounds challenging,",
    "That's really valuable experience,"
]

TRANSITION_PHRASES = [
    "Building on that,",
    "Following up on what you mentioned,",
    "I'd love to hear more about",
    "That brings up an interesting question:",
    "Speaking of that topic,",
    "That reminds me to ask about",
    "Given your experience with that,",
    "Now I'm curious about",
    "That leads me to wonder",
    "Related to what you just shared,"
]

ENCOURAGEMENT_PHRASES = [
    "That's exactly the kind of thinking we're looking for.",
    "Great explanation - you made that very clear.",
    "I really appreciate the depth of your answer.",
    "That shows excellent problem-solving skills.",
    "Your approach to that challenge is impressive.",
    "I can tell you've thought deeply about this.",
    "That's a sophisticated way to handle that situation.",
    "Your experience really comes through in that answer.",
    "That demonstrates strong technical judgment.",
    "I love how you broke that down for me."
]

CLARIFICATION_PROMPTS = [
    "I want to make sure I understand correctly - could you elaborate on that?",
    "That's an interesting point. Can you walk me through that in a bit more detail?",
    "I'd love to hear more about your thinking process there.",
    "Could you give me a specific example of what you mean?",
    "Help me understand the context around that decision.",
    "What was your reasoning behind that approach?",
    "Can you break that down for me step by step?",
    "I'm curious about the details of how you handled that.",
    "What factors did you consider when making that choice?",
    "Could you paint a clearer picture of that situation for me?"
]

GENTLE_REDIRECT_PROMPTS = [
    "That's helpful context. Let me ask you about something related:",
    "I appreciate that background. Now I'm wondering about",
    "That gives me good insight. Building on that topic,",
    "Thanks for that explanation. Let's explore another aspect:",
    "That's valuable information. I'd also like to understand",
    "Good point. Let me shift gears slightly and ask about",
    "That makes sense. On a related note,",
    "I see what you mean. Let me ask you something connected to that:",
    "That's useful context. Now I'm curious about",
    "Thanks for sharing that. Let's dive into another area:"
]

def build_stage_prompt(stage: str, content_context: str = "") -> str:
    stage_prompts = {
        "greeting": GREETING_INTERVIEWER_PROMPT,
        "technical": TECHNICAL_INTERVIEWER_PROMPT,
        "communication": COMMUNICATION_INTERVIEWER_PROMPT,
        "hr": HR_BEHAVIORAL_INTERVIEWER_PROMPT
    }
    base_prompt = stage_prompts.get(stage, TECHNICAL_INTERVIEWER_PROMPT)
    if content_context:
        base_prompt += (
            f"\n\nCANDIDATE'S RECENT WORK CONTEXT:\n{content_context}\n\n"
            "Use this context to ask relevant, personalized questions about their actual work and projects."
        )
    return base_prompt

def build_conversation_prompt(stage: str, user_response: str, content_context: str, conversation_history: str) -> str:
    trimmed_context = content_context[:500] + "..." if len(content_context) > 500 else content_context
    trimmed_history = conversation_history[-1000:] if len(conversation_history) > 1000 else conversation_history
    return CONVERSATION_PROMPT_TEMPLATE.format(
        stage=stage,
        user_response=user_response,
        content_context=trimmed_context,
        conversation_history=trimmed_history
    )

def build_evaluation_prompt(student_name: str, duration: float, stages_completed: list, conversation_log: str, content_context: str) -> str:
    trimmed_context = content_context[:800] + "..." if len(content_context) > 800 else content_context
    return EVALUATION_PROMPT_TEMPLATE.format(
        student_name=student_name,
        duration=f"{duration:.1f}",
        stages_completed=", ".join(stages_completed),
        conversation_log=conversation_log,
        content_context=trimmed_context
    )

def validate_prompts() -> bool:
    prompts_to_check = [
        SYSTEM_CONTEXT_BASE,
        GREETING_INTERVIEWER_PROMPT,
        TECHNICAL_INTERVIEWER_PROMPT,
        COMMUNICATION_INTERVIEWER_PROMPT,
        HR_BEHAVIORAL_INTERVIEWER_PROMPT,
        CONVERSATION_PROMPT_TEMPLATE,
        EVALUATION_PROMPT_TEMPLATE,
        SCORING_PROMPT_TEMPLATE
    ]
    for i, prompt in enumerate(prompts_to_check):
        if not prompt or len(prompt.strip()) < 50:
            raise ValueError(f"Prompt {i} is invalid or too short")
    return True

# Validate on import (matches previous behavior for weekly_interview)
validate_prompts()

__all__ = [
    # Daily standup
    "DailyStandupPrompts", "Prompts", "prompts",
    # Weekend mocktest
    "PromptTemplates", "PromptValidator",
    # Weekly interview
    "SYSTEM_CONTEXT_BASE", "GREETING_INTERVIEWER_PROMPT", "TECHNICAL_INTERVIEWER_PROMPT",
    "COMMUNICATION_INTERVIEWER_PROMPT", "HR_BEHAVIORAL_INTERVIEWER_PROMPT",
    "CONVERSATION_PROMPT_TEMPLATE", "EVALUATION_PROMPT_TEMPLATE", "SCORING_PROMPT_TEMPLATE",
    "ACKNOWLEDGMENT_PHRASES", "TRANSITION_PHRASES", "ENCOURAGEMENT_PHRASES",
    "CLARIFICATION_PROMPTS", "GENTLE_REDIRECT_PROMPTS",
    "build_stage_prompt", "build_conversation_prompt", "build_evaluation_prompt",
    "validate_prompts",
]